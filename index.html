<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hemant Kumawat </title> <meta name="author" content="Hemant Kumawat"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile.png?df1aaaac10cbbc5bca572b1a60d2d5e7"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kumawathemant.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Hemant</span> Kumawat </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile.png" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/profile.png?df1aaaac10cbbc5bca572b1a60d2d5e7" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="profile.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am a fifth-year Ph.D. candidate in Electrical and Computer Engineering at Georgia Tech, advised by <a href="https://www.ece.gatech.edu/faculty-staff-directory/saibal-mukhopadhyay" rel="external nofollow noopener" target="_blank">Prof. Dr. Saibal Mukhopadhyay</a>. My current research focuses on leveraging <em>scarce, offline data collected from robotics and human demonstrations, including suboptimal policies, to develop efficient and adaptive robot learning algorithms with strong learning-theoretic guarantees</em>.</p> <p>My research sits at the intersection of <strong>deep learning, probabilistic modeling, and reinforcement learning</strong>, equipping me to tackle these challenges from multiple angles. One branch of my work enhances robotic sensing and perception by treating it as a closed-loop system, enabling robots to introspect and dynamically balance perception quality, task performance, and resource consumption. Another branch focuses on developing adaptive learning algorithms that efficiently learn from suboptimal offline data, leading to a better understanding of system dynamics and improved decision-making in real-world scenarios. This unique positioning enables me to bridge these paradigms, transforming today‚Äôs powerful models into more general, adaptable, and reasoning-driven autonomous agents capable of making robust decisions in complex, real-world environments.</p> <p>Prior to joining Georgia Tech, I was a part of the SeDriCa team at IIT Bombay, where I worked on the end-to-end software design for an autonomous vehicle, covering everything from perception to control.</p> <p>Feel free to reach me at: hkumawat6[at]gatech.edu</p> </div> <h2 class="academic-research-heading">Academic Research Highlights</h2> <div class="table table-cv table-sm table-borderless table-responsive table-cv-map academic"> <div class="academic-row align-items-center"> <div class="image logo-small"> <span></span> </div> <div class="about-text"> <p></p> </div> </div> <div class="academic-row align-items-center"> <div class="image logo-small"> <img src="assets/img/gatech.jpg" alt="LTU Logo"> </div> <div class="about-text"> <p>PhD candidate at <a href="https://ece.gatech.edu/directory/saibal-mukhopadhyay" rel="external nofollow noopener" target="_blank"> at ECE, Georgia Institute of Technology under Dr. Saibal Mukhopadhyay</a>. Working on learning robust causal representations for efficient robot learning with emphasis on adaptive and generalizable task conditioned spatiotemporal representations for robot learning from partial observations. Published <a href="https://prakashchhipa.github.io/publications/" rel="external nofollow noopener" target="_blank">research works</a> in top robotic conferences including CORL, L4DC, AAMAS, IMS and IJCNN. Have been a reviewer at NeurIPS, ICLR, AISTATS and other conferences. (2021-2025)</p> </div> </div> <div class="academic-row align-items-center"> <div class="image logo-small"> <img src="assets/img/amazon.jpeg" alt="CRCV Logo"> </div> <div class="about-text"> <p>Applied Scientist II Intern at <a href="https://amazon.jobs/content/en/teams/ftr/amazon-robotics" rel="external nofollow noopener" target="_blank"> Amazon Robotics, Boston office. nCenter for Research in Computer Vision (CRCV), University of Central Florida, USA</a>. Mentored by <a href="https://www.linkedin.com/in/andreas-kolling-phd-2767591" rel="external nofollow noopener" target="_blank"> Dr. Andreas Kolling </a>, Jaimie Carlson, and Yulin Zhang. Worked on goal-conditioned multiagent forecasting model to predict slowdowns and deadlocks in Proteus ground robots within a multi-robot warehouse environment. (Summer 2024)</p> </div> </div> <div class="academic-row align-items-center"> <div class="image logo-small"> <img src="assets/img/cmu.jpg" alt="mix Logo"> </div> <div class="about-text"> <p>Visiting Researcher at Robotics Institute, Carnegie Mellon University, Pittsburgh. Guided by host Prof. John M Dolan. Developed motion planning and control algorithms that can utilize complex maneuvers such as drifting in order to equip autonomous vehicles to effectively plan and execute evasive maneuvers (May-Aug 2019)</p> </div> </div> <div class="academic-row align-items-center"> <div class="image logo-small"> <img src="assets/img/sedrica.png" alt="misx Logo"> </div> <div class="about-text"> <p>Technical Student Lead at &lt; a href = 'https://umiciitb.com/'&gt; SeDriCa, Self Driving Car Team | UMIC IIT Bombay . Led a team (top 11 teams out of 259) of over 25+ students to build India‚Äôs 1st Level 5 autonomy car in a 5-tier challenge (prize money- $1 million) (awarded Mahindra e2o electric vehicle). Developed dynamic object detection and tracking architecture for an autonomous vehicle with sensor information from LiDARs, Radars, Cameras, GPS &amp; IMU using grid-based Bayesian Occupation Filter (2018-2020)</p> </div> </div> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Dec 22, 2024</th> <td> üéâ Paper titled <a href="https://arxiv.org/abs/2412.15427" rel="external nofollow noopener" target="_blank">‚ÄòAdaCred: Adaptive Causal Decision Transformers with Feature Crediting_‚Äô</a> accepted in AAMAS 2025. üéâ </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 05, 2024</th> <td> ‚úàÔ∏è I will be attending CORL 2024 in Munich. If you are around, let‚Äôs catch up. ‚úàÔ∏è </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 05, 2024</th> <td> üéâ Paper titled <a href="https://www.arxiv.org/abs/2409.03107" rel="external nofollow noopener" target="_blank">‚ÄòRoboKoop: Efficient Control Conditioned Representations from Visual Input in Robotics using Koopman Operator‚Äô</a> accepted in CORL 2024. üéâ </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 01, 2024</th> <td> üéâ Paper titled <a href="https://arxiv.org/abs/2401.14522" rel="external nofollow noopener" target="_blank">‚ÄòSTEMFold: Stochastic Temporal Manifold for Multi-Agent Interactions in the Presence of Hidden Agents‚Äô</a> accepted in L4DC 2024. üéâ Attending L4DC 2024 in Oxford, UK. If you are around, let‚Äôs catch up! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 07, 2024</th> <td> I am reviewing for Neurips 2024, ICLR 2025, ICML 2025, AISTATS 2024 and IJCNN 2024. </td> </tr> </table> </div> </div> <h2> <a href="/blog/" style="color: inherit">latest posts</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 04, 2024</th> <td> <a class="news-title" href="/blog/2024/kan/">Kolmogorov-Arnold Networks (KAN) (My Notes)</a> </td> </tr> <tr> <th scope="row" style="width: 20%">May 16, 2024</th> <td> <a class="news-title" href="/blog/2024/rl/">Thoughts on RL challenges</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 20, 2024</th> <td> <a class="news-title" href="/blog/2024/benchmark/">Common Tools in Reinforcement Learning for Benchmarking</a> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dac.png" sizes="200px"> <img src="/assets/img/publication_preview/dac.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dac.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="trivedi2025intelligentsensingtoactionrobustautonomy" class="col-sm-8"> <div class="title">Intelligent Sensing-to-Action for Robust Autonomy at the Edge: Opportunities and Challenges</div> <div class="author"> Amit Ranjan Trivedi,¬†<em>Hemant Kumawat</em>,¬†Sina Tayebati, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Nastaran Darabi, Divake Kumar, Adarsh Kumar Kosta, Yeshwanth Venkatesha, Dinithi Jayasuriya, Nethmi Jayasinghe, Priyadarshini Panda, Saibal Mukhopadhyay, Kaushik Roy' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>To appear in Design Automation and Test in Europe (DATE) 2025</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2502.02692" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Autonomous edge computing in robotics, smart cities, and autonomous vehicles relies on the seamless integration of sensing, processing, and actuation for real-time decision-making in dynamic environments. At its core is the sensing-to-action loop, which iteratively aligns sensor inputs with computational models to drive adaptive control strategies. These loops can adapt to hyper-local conditions, enhancing resource efficiency and responsiveness, but also face challenges such as resource constraints, synchronization delays in multi-modal data fusion, and the risk of cascading errors in feedback loops. This article explores how proactive, context-aware sensing-to-action and action-to-sensing adaptations can enhance efficiency by dynamically adjusting sensing and computation based on task demands, such as sensing a very limited part of the environment and predicting the rest. By guiding sensing through control actions, action-to-sensing pathways can improve task relevance and resource use, but they also require robust monitoring to prevent cascading errors and maintain reliability. Multi-agent sensing-action loops further extend these capabilities through coordinated sensing and actions across distributed agents, optimizing resource use via collaboration. Additionally, neuromorphic computing, inspired by biological systems, provides an efficient framework for spike-based, event-driven processing that conserves energy, reduces latency, and supports hierarchical control‚Äìmaking it ideal for multi-agent optimization. This article highlights the importance of end-to-end co-design strategies that align algorithmic models with hardware and environmental dynamics and improve cross-layer interdependencies to improve throughput, precision, and adaptability for energy-efficient edge autonomy in complex environments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cvpr25.png" sizes="200px"> <img src="/assets/img/publication_preview/cvpr25.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cvpr25.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="cvpr25" class="col-sm-8"> <div class="title">Adaptive Feature Diffusion for Low Compute LiDAR Object Detection</div> <div class="author"> Meilong Zhang,¬†<em>Hemant Kumawat</em>,¬†and¬†Saibal Mukhopadhyay </div> <div class="periodical"> <em>Under review at CVPR 2025</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p></p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/aamas.png" sizes="200px"> <img src="/assets/img/publication_preview/aamas.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="aamas.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="9448387" class="col-sm-8"> <div class="title">AdaCred: Adaptive Causal Decision Transformers with Feature Crediting</div> <div class="author"> <em>Hemant Kumawat</em>,¬†and¬†Saibal Mukhopadhyay </div> <div class="periodical"> <em>Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Reinforcement learning (RL) can be viewed as a sequence modeling challenge, where the goal is to predict future actions based on past state-action-reward sequences. Traditional methods often rely on long trajectory sequences to capture environmental dynamics in offline RL scenarios. However, this can lead to a tendency to overemphasize the memorization of long-term representations, which hinders the models‚Äô ability to prioritize trajectories and learned representations that are specifically relevant to the task at hand. In this study, we present AdaDT, a novel approach that conceptualizes trajectories as causal graphs derived from short-term action-reward-state sequences. Our model dynamically adapts its control policy by identifying and eliminating low-importance representations, focusing instead on those that are most pertinent to the downstream task. Experimental results show that policies based on AdaDT require shorter trajectory sequences and consistently outperform traditional methods in both offline reinforcement learning and imitation learning settings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/robokoop.png" sizes="200px"> <img src="/assets/img/publication_preview/robokoop.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="robokoop.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="anonym" class="col-sm-8"> <div class="title">RoboKoop: Efficient Control Conditioned Representations from Visual Input in Robotics using Koopman Operator</div> <div class="author"> <em>Hemant Kumawat</em>,¬†Biswadeep Chakraborty,¬†and¬†Saibal Mukhopadhyay </div> <div class="periodical"> <em>In 62024 Conference on Robot Learning</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/html/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>Developing agents that can perform complex control tasks from high- dimensional observations is a core ability of autonomous agents that requires un- derlying robust task control policies and adapting the underlying visual represen- tations to the task. Most existing policies need a lot of training samples and treat this problem from the lens of two-stage learning with a controller learned on top of pre-trained vision models. We approach this problem from the lens of Koopman theory and learn visual representations from robotic agents conditioned on spe- cific downstream tasks in the context of learning stabilizing control for the agent. We introduce a Contrastive Spectral Koopman Embedding network that allows us to learn efficient linearized visual representations from the agent‚Äôs visual data in a high dimensional latent space and utilizes reinforcement learning to perform off-policy control on top of the extracted representations with a linear controller. Our method enhances stability and control in gradient dynamics over time, signif- icantly outperforming existing approaches by improving efficiency and accuracy in learning task policies over extended horizons.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ims.png" sizes="200px"> <img src="/assets/img/publication_preview/ims.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ims.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10600387" class="col-sm-8"> <div class="title">ChirpNet: Noise-Resilient Sequential Chirp Based Radar Processing for Object Detection</div> <div class="author"> Sudarshan Sharma<sup>**</sup>,¬†<em>Hemant Kumawat<sup>**</sup></em>,¬†and¬†Saibal Mukhopadhyay </div> <div class="periodical"> <em>In 2024 IEEE/MTT-S International Microwave Symposium - IMS 2024 (**equal contribution)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/10600387" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Radar-based object detection (OD) requires extensive pre-processing and complex Machine Learning (ML) pipelines. Previous approaches have attempted to address these challenges by processing raw radar data frames directly from the ADC or through FFT-based post-processing. However, the input data requirements and model complexity continue to impose significant computational overhead on the edge system. In this work, we introduce ChirpNet, a noise-resilient and efficient radar processing ML architecture for object detection. Diverging from previous approaches, we directly handle raw ADC data from multiple antennas per chirp using a sequential model, resulting in a substantial 15 √ó reduction in complexity and a 3 √ó reduction in latency, while maintaining competitive OD performance. Furthermore, our proposed scheme is robust to input noise variations compared to prior works.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/l4dc_model1.png" sizes="200px"> <img src="/assets/img/publication_preview/l4dc_model1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="l4dc_model1.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:conf/l4dc/KumawatCM24" class="col-sm-8"> <div class="title">STEMFold: Stochastic temporal manifold for multi-agent interactions in the presence of hidden agents</div> <div class="author"> <em>Hemant Kumawat</em>,¬†Biswadeep Chakraborty,¬†and¬†Saibal Mukhopadhyay </div> <div class="periodical"> <em>In 6th Annual Learning for Dynamics &amp; Control Conference, 15-17 July 2024, University of Oxford, Oxford, UK</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v242/kumawat24a/kumawat24a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Learning accurate, data-driven predictive models for multiple interacting agents following un- known dynamics is crucial in many real-world physical and social systems. In many scenarios, dynamics prediction must be performed under incomplete observations, i.e., only a subset of agents are known and observable from a larger topological system while the behaviors of the unobserved agents and their interactions with the observed agents are not known. When only incomplete obser- vations of a dynamical system are available, so that some states remain hidden, it is generally not possible to learn a closed-form model in these variables using either analytic or data-driven tech- niques. In this work, we propose STEMFold, a spatiotemporal attention-based generative model, to learn a stochastic manifold to predict the underlying unmeasured dynamics of the multi-agent system from observations of only visible agents. Our analytical results motivate STEMFold design using a spatiotemporal graph with time anchors to effectively map the observations of visible agents to a stochastic manifold with no prior information about interaction graph topology. We empirically evaluated our method on two simulations and two real-world datasets, where it outperformed exist- ing networks in predicting complex multiagent interactions, even with many unobserved agents.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/l4dc_model1.png" sizes="200px"> <img src="/assets/img/publication_preview/l4dc_model1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="l4dc_model1.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="stage" class="col-sm-8"> <div class="title">STAGE Net: Spatio-Temporal Attention-based Graph Encoding for Learning Multi-Agent Interactions in the presence of Hidden Agents</div> <div class="author"> <em>Hemant Kumawat</em>,¬†Biswadeep Chakraborty,¬†and¬†Saibal Mukhopadhyay </div> <div class="periodical"> <em>In Under Review at ICLR 2023</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=tsj6rDzI0V" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Accurate prediction of trajectories for multiple interacting agents following unknown dynamics is crucial in many real-world critical physical and social systems where a group of agents interact with each other, leading to intricate behavior patterns at both the individual and system levels. In many scenarios, trajectory predictions must be performed under partial observations i.e., only a subset of agents are known and observable. Consequently, we can only observe the trajectories of a subset of agents with a sampled interaction graph from a larger topological system while the behaviors of the unobserved agents and their interactions with the observed agents are not known. In this work, we propose STAGE Net, a sequential spatiotemporal attention-based generative model to learn system dynamics with multiple interacting agents where some agents are completely unobserved (hidden) all the time. Our network utilizes the spatiotemporal attention mechanism with neural inter-node messaging to capture high-level behavioral semantics of the multi-agent system. Our analytical results motivate STAGE Net design using spatiotemporal graph with time anchors to effectively model complex multi-agent interactions with unobserved agents and no prior information about interaction graph topology. We evaluate our method on multiagent simulations with spring and charged dynamics and a motion trajectory dataset. Empirical results illustrate that our method outperforms existing multiagent interaction modeling networks in predicting trajectories of complex multiagent interactions even when we have a large number of unobserved agents.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/date.png" sizes="200px"> <img src="/assets/img/publication_preview/date.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="date.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10546823" class="col-sm-8"> <div class="title">Cognitive Sensing for Energy-Efficient Edge Intelligence</div> <div class="author"> Minah Lee,¬†Sudarshan Sharma,¬†Wei Chun Wang, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Hemant Kumawat, Nael Mizanur Rahman, Saibal Mukhopadhyay' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2024 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/10546823" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Edge platforms in autonomous systems integrate multiple sensors to interpret their environment. The high-resolution and high-bandwidth pixel arrays of these sensors improve sensing quality but also generate a vast, and arguably unnecessary, volume of real-time data. This challenge, often referred to as the analog data deluge, hinders the deployment of high-quality sensors in resource-constrained environments. This paper discusses the concept of cognitive sensing, which learns to extract low-dimensional features directly from high-dimensional analog signals, thereby reducing both digitization power and generated data volume. First, we discuss design methods for analog-to-feature extraction (AFE) using mixed-signal compute-in-memory. We then present examples of cognitive sensing, incorporating signal processing or machine learning, for various sensing modalities including vision, Radar, and Infrared. Subsequently, we discuss the reliability challenges in cognitive sensing, taking into account hardware and algorithmic properties of AFE. The paper concludes with discussions on future research directions in this emerging field of cognitive sensors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ijcnn1.png" sizes="200px"> <img src="/assets/img/publication_preview/ijcnn1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ijcnn1.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="9892390" class="col-sm-8"> <div class="title">A Methodology for Understanding the Origins of False Negatives in DNN Based Object Detectors</div> <div class="author"> Kruttidipta Samal,¬†<em>Hemant Kumawat</em>,¬†Marilyn Wolf, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Saibal Mukhopadhyay' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2022 International Joint Conference on Neural Networks (IJCNN)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9892390" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In this paper we present two novel complimentary methods namely the gradient analysis and the activation discrepancy analysis to analyze the perception failures occurring inside the DNN based object detectors. The gradient analysis localizes the nodes within the network that fail consistently in a scenario, thus creating a ‚Äòsignature‚Äô of False Negatives (FNs). This method traces a set of False Negatives through the network and finds sections of the network that contribute to this set. The signatures show the location of the faulty nodes is sensitive to input conditions (such as darkness, glare etc.), network architecture, training hyperparameters, object class etc. Certain nodes of the network fail consistently throughout the training process thus implying that some False Negatives occur due to the global optimization nature of Stochastic Gradient Descent (SGD) based training. This analysis requires the knowledge of False Negatives and therefore can be used for post-hoc diagnostic analysis. On the other hand, the activation discrepancy analysis analyzes the discrepancy in forward activations of a DNN. This method can be conducted online and shows that the pattern of the activation discrepancy is sensitive to input conditions and detection recall.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ijcnn2.png" sizes="200px"> <img src="/assets/img/publication_preview/ijcnn2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ijcnn2.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="9892184" class="col-sm-8"> <div class="title">Radar Guided Dynamic Visual Attention for Resource-Efficient RGB Object Detection</div> <div class="author"> <em>Hemant Kumawat</em>,¬†and¬†Saibal Mukhopadhyay </div> <div class="periodical"> <em>In 2022 International Joint Conference on Neural Networks (IJCNN)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9892184" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>An autonomous system‚Äôs perception engine must provide an accurate understanding of the environment for it to make decisions. Deep learning based object detection networks experience degradation in the performance and robustness for small and far away objects due to a reduction in object‚Äôs feature map as we move to higher layers of the network. In this work, we propose a novel radar-guided spatial attention for RGB images to improve the perception quality of autonomous vehicles operating in a dynamic environment. In particular, our method improves the perception of small and long range objects, which are often not detected by the object detectors in RGB mode. The proposed method consists of two RGB object detectors, namely the Primary detector and a lightweight Secondary detector. The primary detector takes a full RGB image and generates primary detections. Next, the radar proposal framework creates regions of interest (ROIs) for object proposals by projecting the radar point cloud onto the 2D RGB image. These ROIs are cropped and fed to the secondary detector to generate secondary detections which are then fused with the primary detections via non-maximum suppression. This method helps in recovering the small objects by preserving the object‚Äôs spatial features through an increase in their receptive field. We evaluate our fusion method on the challenging nuScenes dataset and show that our fusion method with SSD-lite as primary and secondary detector improves the baseline primary yolov3 detector‚Äôs recall by 14 % while requiring three times fewer computational resources.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tiv.png" sizes="200px"> <img src="/assets/img/publication_preview/tiv.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tiv.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="9448388" class="col-sm-8"> <div class="title">Task-Driven RGB-Lidar Fusion for Object Tracking in Resource-Efficient Autonomous System</div> <div class="author"> Kruttidipta Samal,¬†<em>Hemant Kumawat</em>,¬†Priyabrata Saha, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Marilyn Wolf, Saibal Mukhopadhyay' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Intelligent Vehicles</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/9448387" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Autonomous mobile systems such as vehicles or robots are equipped with multiple sensor modalities including Lidar, RGB, and Radar. The fusion of multi-modal information can enhance task accuracy but indiscriminate sensing and fusion in all modalities increase demand on available system resources. This paper presents a task-driven approach to input fusion that minimizes the utilization of resource-heavy sensors and demonstrates its application to Visual-Lidar fusion for object tracking and path planning. The proposed spatiotemporal sampling algorithm activates Lidar only at regions-of-interest identified by analyzing visual input and reduces the Lidar ‚Äòbase frame rate‚Äô according to the kinematic state of the system. This significantly reduces Lidar usage, in terms of data sensed/transferred and potentially power consumed, without a severe reduction in performance compared to both a baseline decision-level fusion and state-of-the-art deep multi-modal fusion.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%68%6B%75%6D%61%77%61%74%36@%67%61%74%65%63%68.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=2iUnwBwAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/kumawathemant" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/kumawathemant" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/HeMan553" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note">Feel free to reach me at: hkumawat6[at]gatech.edu </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Hemant Kumawat. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-research",title:"Research",description:"A growing collection of my research projects.",section:"Navigation",handler:()=>{window.location.href="/research/"}},{id:"nav-cv",title:"CV",description:"Email me at hkumawat6@gatech.edu.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-kolmogorov-arnold-networks-kan-my-notes",title:"Kolmogorov-Arnold Networks (KAN) (My Notes)",description:"Review of Kolmogorov-Arnold Networks (KAN) and their mathematical foundation.",section:"Posts",handler:()=>{window.location.href="/blog/2024/kan/"}},{id:"post-thoughts-on-rl-challenges",title:"Thoughts on RL challenges",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/rl/"}},{id:"post-common-tools-in-reinforcement-learning-for-benchmarking",title:"Common Tools in Reinforcement Learning for Benchmarking",description:"A guide to the most popular tools used in reinforcement learning for benchmarking algorithms.",section:"Posts",handler:()=>{window.location.href="/blog/2024/benchmark/"}},{id:"post-stuck-at-installing-mujoco-use-this-guide",title:"Stuck at Installing Mujoco? Use this guide!!",description:"A step-by-step guide to installing Mujoco on your system.",section:"Posts",handler:()=>{window.location.href="/blog/2024/mujoco/"}},{id:"post-are-current-robotics-methods-deployable-on-edge-robots",title:"Are Current Robotics Methods Deployable on Edge Robots?",description:"A discussion on the challenges of deploying state-of-the-art robotics algorithms on edge robots.",section:"Posts",handler:()=>{window.location.href="/blog/2024/robot/"}},{id:"post-neural-ode-solvers-my-notes",title:"Neural ODE Solvers (My Notes)",description:"Some notes on ML for solving ordinary differential equations.",section:"Posts",handler:()=>{window.location.href="/blog/2022/ode/"}},{id:"news-i-am-reviewing-for-neurips-2024-iclr-2025-icml-2025-aistats-2024-and-ijcnn-2024",title:"I am reviewing for Neurips 2024, ICLR 2025, ICML 2025, AISTATS 2024 and...",description:"",section:"News"},{id:"news-paper-titled-stemfold-stochastic-temporal-manifold-for-multi-agent-interactions-in-the-presence-of-hidden-agents-accepted-in-l4dc-2024-attending-l4dc-2024-in-oxford-uk-if-you-are-around-let-s-catch-up",title:"\ud83c\udf89 Paper titled \u2018STEMFold: Stochastic Temporal Manifold for Multi-Agent Interactions in the Presence...",description:"",section:"News"},{id:"news-paper-titled-robokoop-efficient-control-conditioned-representations-from-visual-input-in-robotics-using-koopman-operator-accepted-in-corl-2024",title:"\ud83c\udf89 Paper titled \u2018RoboKoop: Efficient Control Conditioned Representations from Visual Input in Robotics...",description:"",section:"News"},{id:"news-\ufe0f-i-will-be-attending-corl-2024-in-munich-if-you-are-around-let-s-catch-up-\ufe0f",title:"\u2708\ufe0f I will be attending CORL 2024 in Munich. If you are around,...",description:"",section:"News"},{id:"news-paper-titled-adacred-adaptive-causal-decision-transformers-with-feature-crediting-accepted-in-aamas-2025",title:"\ud83c\udf89 Paper titled \u2018AdaCred: Adaptive Causal Decision Transformers with Feature Crediting_\u2019 accepted in...",description:"",section:"News"},{id:"projects-stochastic-temporal-manifold-for-multi-agent-interactions",title:"Stochastic Temporal Manifold for Multi-Agent Interactions",description:"Learning dynamics in  partial/incomplete observability scenarios",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%6B%75%6D%61%77%61%74%36@%67%61%74%65%63%68.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=2iUnwBwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/kumawathemant","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/kumawathemant","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/HeMan553","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>