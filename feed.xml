<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kumawathemant.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kumawathemant.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-23T21:40:07+00:00</updated><id>https://kumawathemant.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Are Current Robotics Methods Deployable on Edge Robots?</title><link href="https://kumawathemant.github.io/blog/2024/robot/" rel="alternate" type="text/html" title="Are Current Robotics Methods Deployable on Edge Robots?"/><published>2024-01-03T04:15:00+00:00</published><updated>2024-01-03T04:15:00+00:00</updated><id>https://kumawathemant.github.io/blog/2024/robot</id><content type="html" xml:base="https://kumawathemant.github.io/blog/2024/robot/"><![CDATA[ <p>As edge robotics advances, one central question persists: <strong>Can we directly deploy current state-of-the-art robotics methods on edge devices, or do they need significant adaptation?</strong> Edge robots, defined by their capability to process data locally with limited resources, face unique challenges that complicate the use of traditional robotics algorithms. In this post, we delve into why existing methods often fall short and explore the latest research on necessary adaptations for effective deployment.</p> <h2 id="1-the-limitations-of-edge-robotics">1. The Limitations of Edge Robotics</h2> <p>Edge robots, like autonomous drones, mobile robots, and wearable robotic systems, must operate with limited resources in unpredictable environments. The primary constraints include:</p> <ul> <li><strong>Restricted Computation</strong>: Unlike server-based systems, edge robots lack high-powered GPUs or large memory banks.</li> <li><strong>Energy Constraints</strong>: Most edge robots rely on batteries, limiting the power available for processing.</li> <li><strong>Real-Time Demands</strong>: Tasks such as navigation, mapping, and object detection require near-instant responses, often under severe latency constraints.</li> </ul> <h3 id="current-methods-bottlenecks-in-deployability">Current Methods: Bottlenecks in Deployability</h3> <p>The majority of state-of-the-art algorithms in robotics assume ample computational resources, making direct deployment on edge robots problematic. Below, we explore the specific issues with some of the most commonly used techniques.</p> <h2 id="2-the-challenges-with-current-robotics-algorithms">2. The Challenges with Current Robotics Algorithms</h2> <h3 id="deep-reinforcement-learning-drl">Deep Reinforcement Learning (DRL)</h3> <p><strong>Deep Reinforcement Learning (DRL)</strong> is a popular method for decision-making and navigation. However, several studies highlight the <strong>resource intensity</strong> of DRL:</p> <ul> <li> <p><strong>Memory Usage</strong>: DRL models often require millions of parameters to learn complex policies, which demands memory resources far beyond the capabilities of most edge devices. Deploying DRL-based policies without compression can lead to significant slowdowns, impacting real-time performance [^1].</p> </li> <li> <p><strong>Inference Latency</strong>: DRL algorithms rely on deep neural networks that demand high inference speeds, especially in safety-critical tasks. Without access to GPUs, processing speeds are insufficient for real-time decision-making, leading to unreliable behavior.</p> </li> </ul> <h3 id="simultaneous-localization-and-mapping-slam">Simultaneous Localization and Mapping (SLAM)</h3> <p><strong>Simultaneous Localization and Mapping (SLAM)</strong> is fundamental for creating maps of unknown environments while tracking a robot’s position. Modern SLAM techniques use a combination of <strong>Visual SLAM (V-SLAM)</strong> and <strong>Lidar-based SLAM</strong>:</p> <ul> <li> <p><strong>Computational Complexity</strong>: State-of-the-art SLAM algorithms involve dense feature extraction, point-cloud registration, and optimization steps that are computationally intensive. V-SLAM algorithms struggle to achieve adequate frame rates on edge devices without aggressive downsampling, affecting map accuracy <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">1</a></sup>.</p> </li> <li> <p><strong>Energy Drain</strong>: SLAM requires intensive use of sensors like LiDAR and stereo cameras, leading to rapid battery depletion. Energy-efficient SLAM alternatives often sacrifice mapping accuracy, creating a trade-off between precision and operational duration <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">2</a></sup>.</p> </li> </ul> <h3 id="large-scale-neural-networks-for-vision">Large-Scale Neural Networks for Vision</h3> <p>Robotics applications rely heavily on <strong>Convolutional Neural Networks (CNNs)</strong> for tasks like object detection, segmentation, and classification. State-of-the-art models such as ResNet, YOLO, and Mask R-CNN pose challenges on edge devices:</p> <ul> <li> <p><strong>Model Size</strong>: Modern CNNs can have millions of parameters. A standard YOLOv5 model, for example, has over 7 million parameters, consuming several hundred MBs of memory, far exceeding the storage capacity of most edge platforms.</p> </li> <li> <p><strong>Inference Throughput</strong>: Deploying large-scale vision models on edge hardware results in substantial inference slowdowns, making them impractical for time-sensitive applications <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">3</a></sup>.</p> </li> </ul> <h3 id="model-predictive-control-mpc-and-optimal-control">Model Predictive Control (MPC) and Optimal Control</h3> <p><strong>Model Predictive Control (MPC)</strong> is a widely-used method for trajectory planning and control in robotics:</p> <ul> <li> <p><strong>Computational Demand</strong>: MPC involves solving optimization problems at every control step, which is computationally prohibitive on low-power CPUs. Real-time MPC for high-dimensional systems requires dedicated hardware, which many edge devices lack <sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup>.</p> </li> <li> <p><strong>Sensitivity to Disturbances</strong>: Adaptive MPC requires significant computational overhead to handle environmental uncertainties, which is challenging on edge platforms without sacrificing control performance.</p> </li> </ul> <h2 id="3-recent-research-on-adaptations-for-edge-deployability">3. Recent Research on Adaptations for Edge Deployability</h2> <h3 id="model-compression-and-pruning">Model Compression and Pruning</h3> <p><strong>Compression Techniques</strong> like pruning, quantization, and neural architecture search (NAS) have been studied to reduce model sizes:</p> <ul> <li> <p><strong>Pruning</strong> involves removing unnecessary weights from neural networks, reducing memory footprints. Pruning can make complex models feasible for edge platforms, although at the cost of some accuracy <sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup>.</p> </li> <li> <p><strong>Quantization</strong> reduces the precision of weights, decreasing computational load. Quantizing models can result in performance degradation, particularly in tasks requiring fine-grained accuracy, like robotic manipulation <sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">6</a></sup>.</p> </li> </ul> <h3 id="tinyml-creating-edge-specific-models">TinyML: Creating Edge-Specific Models</h3> <p><strong>TinyML</strong> refers to machine learning techniques explicitly designed for resource-constrained devices:</p> <ul> <li> <p>Techniques like <strong>Neural Architecture Search (NAS)</strong> have been proposed to automatically discover lightweight architectures optimized for edge devices. TinyNAS models have demonstrated a significant reduction in computational requirements while maintaining accuracy for specific tasks <sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">7</a></sup>.</p> </li> <li> <p><strong>Spiking Neural Networks (SNNs)</strong>, which mimic the brain’s spike-based information processing, have shown promise for power-efficient robotics. SNNs can achieve comparable performance to traditional CNNs for certain robotic tasks while consuming less power <sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">8</a></sup>.</p> </li> </ul> <h3 id="distributed-learning-for-edge-swarms">Distributed Learning for Edge Swarms</h3> <p><strong>Federated Learning</strong> is gaining attention for edge robotics, where multiple devices share updates to train models collaboratively without centralizing data:</p> <ul> <li>Federated learning has enabled swarms of drones to coordinate and adapt without sending raw data back to a central server, reducing communication overhead and preserving data privacy <sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">9</a></sup>.</li> </ul> <h2 id="4-the-need-for-new-paradigms-in-robotics-algorithms">4. The Need for New Paradigms in Robotics Algorithms</h2> <p>To meet the unique demands of edge robots, future research must focus on inherently lightweight and adaptive algorithms:</p> <ul> <li><strong>Dynamic Neural Networks</strong> that adjust their complexity based on real-time resource constraints.</li> <li><strong>Event-Driven Processing</strong>, where computations are triggered by events rather than continuously, reducing idle power consumption.</li> <li><strong>Bio-inspired Models</strong> that emulate efficient natural processes, leading to inherently resource-efficient computation.</li> </ul> <h2 id="references">References</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:2" role="doc-endnote"> <p>Mur-Artal, R., &amp; Tardós, J. D. (2017). ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras. <em>IEEE Transactions on Robotics</em>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>Zhang, Q., &amp; Scaramuzza, D. (2020). Energy-Efficient Visual-Inertial Odometry for Micro Aerial Vehicles. <em>IEEE Robotics and Automation Letters</em>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4" role="doc-endnote"> <p>Han, S., Mao, H., &amp; Dally, W. J. (2016). Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. <em>International Conference on Learning Representations (ICLR)</em>. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5" role="doc-endnote"> <p>Kouvaritakis, B., &amp; Cannon, M. (2016). Model Predictive Control: Classical, Robust, and Stochastic. <em>Springer</em>. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6" role="doc-endnote"> <p>He, Y., Lin, J., &amp; Wang, Z. (2018). AMC: AutoML for Model Compression and Acceleration on Mobile Devices. <em>European Conference on Computer Vision (ECCV)</em>. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:7" role="doc-endnote"> <p>Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., &amp; Wu, Y. (2018). Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:8" role="doc-endnote"> <p>Tan, M., &amp; Le, Q. (2019). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. <em>International Conference on Machine Learning (ICML)</em>. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:9" role="doc-endnote"> <p>Tavanaei, A., &amp; Maida, A. (2019). Bio-Inspired Spiking Neural Networks for Object Recognition in Surveillance Systems. <em>Frontiers in Neuroscience</em>. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:10" role="doc-endnote"> <p>Konečný, J., McMahan, H. B., &amp; Ramage, D. (2016). Federated Optimization: Distributed Machine Learning for On-Device Intelligence. <em>arXiv preprint arXiv:1610.02527</em>. <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI"/><category term="ml"/><category term="robot"/><category term="edge-robots"/><summary type="html"><![CDATA[A discussion on the challenges of deploying state-of-the-art robotics algorithms on edge robots.]]></summary></entry><entry><title type="html">Neural ODE Solvers (My Notes)</title><link href="https://kumawathemant.github.io/blog/2022/ode/" rel="alternate" type="text/html" title="Neural ODE Solvers (My Notes)"/><published>2022-10-11T10:25:00+00:00</published><updated>2022-10-11T10:25:00+00:00</updated><id>https://kumawathemant.github.io/blog/2022/ode</id><content type="html" xml:base="https://kumawathemant.github.io/blog/2022/ode/"><![CDATA[<p>The concept of <strong>Neural Ordinary Differential Equations (Neural ODEs)</strong> has emerged as a breakthrough approach for defining neural network architectures using the language of continuous mathematics. Unlike traditional neural networks, which operate in discrete layers, Neural ODEs use a continuous representation, which has applications ranging from time-series prediction to physics-inspired models. The introduction of Neural ODEs was popularized by the seminal paper “Neural Ordinary Differential Equations” by Chen et al. (2018) <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, and it fundamentally changes how we understand neural architectures.</p> <p>In this post, we’ll provide a mathematically rigorous breakdown of Neural ODEs, including their formulation, theoretical properties, training algorithms, and advantages over conventional models. This discussion assumes a background in differential equations, numerical analysis, and neural network theory.</p> <h2 id="classical-neural-networks">Classical Neural Networks</h2> <p>Before diving into Neural ODEs, let’s establish the formalism of classical neural networks. A neural network is typically defined as a sequence of layers, where each layer applies a transformation to the input. In mathematical terms:</p> <p>[ \mathbf{h}_{k+1} = f(\mathbf{h}_k, \theta_k), \quad k = 0, 1, \dots, K-1 ]</p> <p>Here:</p> <ul> <li>( \mathbf{h}_k ) is the hidden state at layer ( k ).</li> <li>( f ) is the transformation applied at layer ( k ), parameterized by ( \theta_k ).</li> <li>( K ) is the total number of layers.</li> </ul> <p>In this formulation, the input ( \mathbf{x} ) is mapped to the final output ( \mathbf{y} = \mathbf{h}_K ) after passing through ( K ) discrete transformations.</p> <h2 id="2-neural-ode-continuous-view-of-neural-networks">2. Neural ODE: Continuous View of Neural Networks</h2> <p>Neural ODEs replace the discrete layer updates with a continuous transformation governed by a system of ordinary differential equations (ODEs). Formally, instead of discrete iterations, we define the hidden state as a continuous function ( \mathbf{h}(t) ), parameterized by a time variable ( t ):</p> <p>[ \frac{d \mathbf{h}(t)}{dt} = f(\mathbf{h}(t), t, \theta) ]</p> <p>Here:</p> <ul> <li>( \mathbf{h}(t) ) is the hidden state, continuously evolving over time.</li> <li>( f(\mathbf{h}(t), t, \theta) ) is a learnable function that specifies the evolution of ( \mathbf{h}(t) ).</li> <li>( \theta ) are the parameters of the neural network that dictate the dynamics.</li> </ul> <h2 id="3-solving-the-ode-flow-of-information">3. Solving the ODE: Flow of Information</h2> <p>The problem then reduces to solving an initial value problem (IVP), where the solution ( \mathbf{h}(t) ) evolves from an initial condition ( \mathbf{h}(t_0) ). Mathematically, this is represented as:</p> <p>[ \mathbf{h}(t_1) = \mathbf{h}(t_0) + \int_{t_0}^{t_1} f(\mathbf{h}(t), t, \theta) \, dt ]</p> <p>In practice, this integral is approximated using numerical ODE solvers such as Euler’s method, Runge-Kutta methods, or adaptive solvers like Dormand-Prince (DOPRI).</p> <h2 id="4-computational-graph-the-adjoints-and-backpropagation">4. Computational Graph: The Adjoints and Backpropagation</h2> <p>A crucial aspect of Neural ODEs is how gradients are computed for training. Unlike standard neural networks, which rely on backpropagation through a discrete computational graph, Neural ODEs employ a <strong>continuous adjoint method</strong>.</p> <p>The adjoint method relies on the principle that solving the ODE forward in time to compute ( \mathbf{h}(t) ) and solving a corresponding backward ODE to compute gradients can be done efficiently. This is achieved using the <strong>adjoint state</strong>, defined as:</p> <p>[ \frac{d \mathbf{a}(t)}{dt} = - \mathbf{a}(t)^T \frac{\partial f(\mathbf{h}(t), t, \theta)}{\partial \mathbf{h}(t)} ]</p> <p>Where:</p> <ul> <li>( \mathbf{a}(t) ) is the adjoint state, analogous to a Lagrange multiplier in control theory.</li> <li>The backward integration computes gradients with respect to parameters ( \theta ).</li> </ul> <p>The total gradient can be expressed as:</p> <p>[ \frac{\partial L}{\partial \theta} = - \int_{t_1}^{t_0} \mathbf{a}(t)^T \frac{\partial f(\mathbf{h}(t), t, \theta)}{\partial \theta} \, dt ]</p> <p>Where ( L ) is the loss function evaluated at the final state ( \mathbf{h}(t_1) ).</p> <h2 id="5-training-neural-odes-adaptive-solvers">5. Training Neural ODEs: Adaptive Solvers</h2> <p>Neural ODEs can leverage adaptive ODE solvers that dynamically adjust the step size for integration, making the computation efficient. The adaptivity enables Neural ODEs to handle complex dynamics with fewer computational resources. The choice of solver impacts the speed and accuracy, balancing trade-offs between precision and computational cost.</p> <h2 id="6-applications-of-neural-odes">6. Applications of Neural ODEs</h2> <p>6.1 Time-Series Modeling Neural ODEs are well-suited for time-series analysis, especially when the data involves continuous trajectories. Examples include financial forecasting and physical system simulations. By treating the hidden state evolution as an ODE, the model can naturally handle irregularly sampled data.</p> <p>6.2. Latent Dynamics in Variational Autoencoders (VAEs) In the context of generative models, Neural ODEs are used to replace the discrete latent dynamics in VAEs. <strong>Continuous Normalizing Flows (CNFs)</strong>, which are based on Neural ODEs, allow for reversible transformations of data, making them effective in density estimation.</p> <h2 id="7-advantages-of-neural-odes">7. Advantages of Neural ODEs</h2> <ol> <li><strong>Memory Efficiency</strong>: Traditional networks store intermediate activations for backpropagation, but Neural ODEs only require the initial state and final state, thanks to the adjoint method.</li> <li><strong>Parameter Efficiency</strong>: The dynamics are governed by a compact parameter set ( \theta ), reducing the overall model size.</li> <li><strong>Smoothness</strong>: The continuous formulation provides a natural smoothness in the data representation, beneficial for problems like trajectory prediction.</li> <li><strong>Adaptive Computation</strong>: By adjusting the solver’s precision, Neural ODEs can balance computational efficiency and accuracy dynamically.</li> </ol> <h2 id="8-mathematical-properties-stability-and-robustness">8. Mathematical Properties: Stability and Robustness</h2> <p>8.1. Stability Analysis The stability of the learned dynamics is crucial for ensuring that solutions do not diverge. For a system defined by ( \frac{d\mathbf{h}}{dt} = f(\mathbf{h}, t, \theta) ), the stability can be analyzed by considering the eigenvalues of the Jacobian matrix ( \frac{\partial f}{\partial \mathbf{h}} ). Stability requires that the real parts of these eigenvalues remain negative.</p> <p>8.2. Regularization via Control Theory To ensure stability and smoothness, one can impose a regularization term in the loss function, penalizing large values of the Jacobian’s norm. This is akin to control theory approaches, where energy constraints are applied to regulate the system’s evolution.</p> <h2 id="9-challenges-and-limitations">9. Challenges and Limitations</h2> <ol> <li><strong>Computational Cost</strong>: While the memory is efficient, the time taken to solve an ODE can be longer due to the iterative nature of numerical solvers.</li> <li><strong>Hyperparameter Sensitivity</strong>: The choice of ODE solver, step size, and regularization parameters heavily influence model behavior.</li> <li><strong>Non-Uniqueness</strong>: A given trajectory can often be represented by multiple sets of dynamics, leading to potential issues with identifiability.</li> </ol> <h2 id="10-extensions-and-future-directions">10. Extensions and Future Directions</h2> <p>10.1. Stochastic Neural ODEs Extensions like <strong>Stochastic Differential Equations (SDEs)</strong> have been proposed to model noise and uncertainty directly. Stochastic Neural ODEs account for randomness in dynamics, making them suitable for modeling noisy data.</p> <p>10.2. Neural ODEs on Manifolds Incorporating manifold constraints into the ODE formulation allows Neural ODEs to model data lying on non-Euclidean spaces, such as graph structures or Riemannian manifolds. This is an emerging research area with potential applications in 3D vision and robotics.</p> <p>10.3. Hamiltonian Neural Networks To model systems with conserved quantities (e.g., energy), Hamiltonian Neural Networks use Neural ODEs with Hamiltonian dynamics. This structure imposes a conservation law, which is beneficial for physics-based modeling.</p> <h2 id="references">References</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Chen, R. T. Q., Rubanova, Y., Bettencourt, J., &amp; Duvenaud, D. K. (2018). Neural Ordinary Differential Equations. In <em>Advances in Neural Information Processing Systems</em>, 6571-6583. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="AI"/><category term="ml"/><category term="ode"/><category term="numerical-methods"/><summary type="html"><![CDATA[Some notes on ML for solving ordinary differential equations.]]></summary></entry></feed>